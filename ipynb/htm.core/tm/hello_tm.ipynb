{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "\n",
      "This program shows how to access the Temporal Memory algorithm directly.  This\n",
      "program demonstrates how to create a TM instance, train it, get predictions and\n",
      "anomalies, and inspect the state.\n",
      "\n",
      "The code here runs a very simple version of sequence learning, with one\n",
      "cell per column. The TM is trained with the simple sequence A->B->C->D->E\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Creating the Temporal Memory\n",
      "Temporal Memory Parameters\n",
      "version                   = 2\n",
      "numColumns                = 50\n",
      "cellsPerColumn            = 1\n",
      "activationThreshold       = 8\n",
      "initialPermanence         = 0.5\n",
      "connectedPermanence       = 0.5\n",
      "minThreshold              = 8\n",
      "maxNewSynapseCount        = 20\n",
      "permanenceIncrement       = 0.1\n",
      "permanenceDecrement       = 0\n",
      "predictedSegmentDecrement = 0\n",
      "maxSegmentsPerCell        = 255\n",
      "maxSynapsesPerSegment     = 255\n",
      "\n",
      "Creating inputs to feed to the temporal memory. Each input is an SDR\n",
      "representing the active mini-columns.  Here we create a simple sequence of 5\n",
      "SDRs representing the sequence A -> B -> C -> D -> E \n",
      "Input: A  Bits: 1111111111 0000000000 0000000000 0000000000 0000000000 \n",
      "Input: B  Bits: 0000000000 1111111111 0000000000 0000000000 0000000000 \n",
      "Input: C  Bits: 0000000000 0000000000 1111111111 0000000000 0000000000 \n",
      "Input: D  Bits: 0000000000 0000000000 0000000000 1111111111 0000000000 \n",
      "Input: E  Bits: 0000000000 0000000000 0000000000 0000000000 1111111111 \n",
      "\n",
      "################################################################################\n",
      "\n",
      "Send this simple sequence to the temporal memory for learning.\n",
      "\n",
      "The compute method performs one step of learning and/or inference. Note: here\n",
      "we just perform learning but you can perform prediction/inference and learning\n",
      "in the same step if you want (online learning).\n",
      "\n",
      "Input: A\n",
      ">>> tm.compute()\n",
      "Active cells     1111111111 0000000000 0000000000 0000000000 0000000000 \n",
      "Winner cells     1111111111 0000000000 0000000000 0000000000 0000000000 \n",
      "Predictive cells 0000000000 0000000000 0000000000 0000000000 0000000000 \n",
      "Anomaly 100.0 %\n",
      "\n",
      "Input: B\n",
      ">>> tm.compute()\n",
      "Active cells     0000000000 1111111111 0000000000 0000000000 0000000000 \n",
      "Winner cells     0000000000 1111111111 0000000000 0000000000 0000000000 \n",
      "Predictive cells 0000000000 0000000000 0000000000 0000000000 0000000000 \n",
      "Anomaly 100.0 %\n",
      "\n",
      "Input: C\n",
      ">>> tm.compute()\n",
      "Active cells     0000000000 0000000000 1111111111 0000000000 0000000000 \n",
      "Winner cells     0000000000 0000000000 1111111111 0000000000 0000000000 \n",
      "Predictive cells 0000000000 0000000000 0000000000 0000000000 0000000000 \n",
      "Anomaly 100.0 %\n",
      "\n",
      "Input: D\n",
      ">>> tm.compute()\n",
      "Active cells     0000000000 0000000000 0000000000 1111111111 0000000000 \n",
      "Winner cells     0000000000 0000000000 0000000000 1111111111 0000000000 \n",
      "Predictive cells 0000000000 0000000000 0000000000 0000000000 0000000000 \n",
      "Anomaly 100.0 %\n",
      "\n",
      "Input: E\n",
      ">>> tm.compute()\n",
      "Active cells     0000000000 0000000000 0000000000 0000000000 1111111111 \n",
      "Winner cells     0000000000 0000000000 0000000000 0000000000 1111111111 \n",
      "Predictive cells 0000000000 0000000000 0000000000 0000000000 0000000000 \n",
      "Anomaly 100.0 %\n",
      "\n",
      "The reset command tells the TM that a sequence just ended and essentially\n",
      "zeros out all the states. It is not strictly necessary but it's a bit\n",
      "messier without resets, and the TM learns quicker with resets.\n",
      "\n",
      ">>> tm.reset()\n",
      "\n",
      "################################################################################\n",
      "\n",
      "Send the same sequence of vectors and look at predictions made by\n",
      "temporal memory.\n",
      "\n",
      "The following prints out the active cells, predictive cells, active segments and\n",
      "winner cells.\n",
      "\n",
      "What you should notice is that the mini-columns where active state is 1\n",
      "represent the SDR for the current input pattern and the columns where predicted\n",
      "state is 1 represent the SDR for the next expected pattern.\n",
      "\n",
      "Input: A\n",
      ">>> tm.compute()\n",
      "Active cells     1111111111 0000000000 0000000000 0000000000 0000000000 \n",
      "Winner cells     1111111111 0000000000 0000000000 0000000000 0000000000 \n",
      "Predictive cells 0000000000 1111111111 0000000000 0000000000 0000000000 \n",
      "Anomaly 100.0 %\n",
      "\n",
      "Input: B\n",
      ">>> tm.compute()\n",
      "Active cells     0000000000 1111111111 0000000000 0000000000 0000000000 \n",
      "Winner cells     0000000000 1111111111 0000000000 0000000000 0000000000 \n",
      "Predictive cells 0000000000 0000000000 1111111111 0000000000 0000000000 \n",
      "Anomaly 0.0 %\n",
      "\n",
      "Input: C\n",
      ">>> tm.compute()\n",
      "Active cells     0000000000 0000000000 1111111111 0000000000 0000000000 \n",
      "Winner cells     0000000000 0000000000 1111111111 0000000000 0000000000 \n",
      "Predictive cells 0000000000 0000000000 0000000000 1111111111 0000000000 \n",
      "Anomaly 0.0 %\n",
      "\n",
      "Input: D\n",
      ">>> tm.compute()\n",
      "Active cells     0000000000 0000000000 0000000000 1111111111 0000000000 \n",
      "Winner cells     0000000000 0000000000 0000000000 1111111111 0000000000 \n",
      "Predictive cells 0000000000 0000000000 0000000000 0000000000 1111111111 \n",
      "Anomaly 0.0 %\n",
      "\n",
      "Input: E\n",
      ">>> tm.compute()\n",
      "Active cells     0000000000 0000000000 0000000000 0000000000 1111111111 \n",
      "Winner cells     0000000000 0000000000 0000000000 0000000000 1111111111 \n",
      "Predictive cells 0000000000 0000000000 0000000000 0000000000 0000000000 \n",
      "Anomaly 0.0 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# HTM Community Edition of NuPIC\n",
    "# Copyright (C) 2013, Numenta, Inc.\n",
    "#               2019, David McDougall\n",
    "#\n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU Affero Public License version 3 as\n",
    "# published by the Free Software Foundation.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
    "# See the GNU Affero Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU Affero Public License\n",
    "# along with this program.  If not, see http://www.gnu.org/licenses.\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "__doc__ = \"\"\"\n",
    "This program shows how to access the Temporal Memory algorithm directly.  This\n",
    "program demonstrates how to create a TM instance, train it, get predictions and\n",
    "anomalies, and inspect the state.\n",
    "\n",
    "The code here runs a very simple version of sequence learning, with one\n",
    "cell per column. The TM is trained with the simple sequence A->B->C->D->E\n",
    "\"\"\"\n",
    "\n",
    "from htm.bindings.sdr import SDR\n",
    "from htm.algorithms import TemporalMemory as TM\n",
    "\n",
    "# Utility routine for printing an SDR in a particular way.\n",
    "def formatBits(sdr):\n",
    "  s = ''\n",
    "  for c in range(sdr.size):\n",
    "    if c > 0 and c % 10 == 0:\n",
    "      s += ' '\n",
    "    s += str(sdr.dense.flatten()[c])\n",
    "  s += ' '\n",
    "  return s\n",
    "\n",
    "def printStateTM( tm ):\n",
    "    # Useful for tracing internal states\n",
    "    print(\"Active cells     \" + formatBits(tm.getActiveCells()))\n",
    "    print(\"Winner cells     \" + formatBits(tm.getWinnerCells()))\n",
    "    tm.activateDendrites(True)\n",
    "    print(\"Predictive cells \" + formatBits(tm.getPredictiveCells()))\n",
    "    print(\"Anomaly\", tm.anomaly * 100, \"%\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "print(\"################################################################################\")\n",
    "print(__doc__)\n",
    "print(\"################################################################################\")\n",
    "print(\"\")\n",
    "print(\"Creating the Temporal Memory\")\n",
    "tm = TM(columnDimensions = (50,),\n",
    "        cellsPerColumn=1,\n",
    "        initialPermanence=0.5,\n",
    "        connectedPermanence=0.5,\n",
    "        minThreshold=8,\n",
    "        maxNewSynapseCount=20,\n",
    "        permanenceIncrement=0.1,\n",
    "        permanenceDecrement=0.0,\n",
    "        activationThreshold=8,\n",
    "        )\n",
    "tm.printParameters()\n",
    "\n",
    "print(\"\"\"\n",
    "Creating inputs to feed to the temporal memory. Each input is an SDR\n",
    "representing the active mini-columns.  Here we create a simple sequence of 5\n",
    "SDRs representing the sequence A -> B -> C -> D -> E \"\"\")\n",
    "dataset = { inp : SDR( tm.numberOfColumns() ) for inp in \"ABCDE\" }\n",
    "dataset['A'].dense[0:10]  = 1   # Input SDR representing \"A\", corresponding to mini-columns 0-9\n",
    "dataset['B'].dense[10:20] = 1   # Input SDR representing \"B\", corresponding to mini-columns 10-19\n",
    "dataset['C'].dense[20:30] = 1   # Input SDR representing \"C\", corresponding to mini-columns 20-29\n",
    "dataset['D'].dense[30:40] = 1   # Input SDR representing \"D\", corresponding to mini-columns 30-39\n",
    "dataset['E'].dense[40:50] = 1   # Input SDR representing \"E\", corresponding to mini-columns 40-49\n",
    "# Notify the SDR object that we've updated its dense data in-place.\n",
    "for z in dataset.values():\n",
    "  z.dense = z.dense\n",
    "for inp in \"ABCDE\":\n",
    "  print(\"Input:\", inp, \" Bits:\", formatBits( dataset[inp]) )\n",
    "print(\"\")\n",
    "\n",
    "print(\"################################################################################\")\n",
    "print(\"\")\n",
    "print(\"\"\"Send this simple sequence to the temporal memory for learning.\"\"\")\n",
    "print(\"\"\"\n",
    "The compute method performs one step of learning and/or inference. Note: here\n",
    "we just perform learning but you can perform prediction/inference and learning\n",
    "in the same step if you want (online learning).\n",
    "\"\"\")\n",
    "for inp in \"ABCDE\": # Send each letter in the sequence in order\n",
    "  print(\"Input:\", inp)\n",
    "  activeColumns = dataset[inp]\n",
    "\n",
    "  print(\">>> tm.compute()\")\n",
    "  tm.compute(activeColumns, learn = True)\n",
    "\n",
    "  printStateTM(tm)\n",
    "\n",
    "print(\"\"\"The reset command tells the TM that a sequence just ended and essentially\n",
    "zeros out all the states. It is not strictly necessary but it's a bit\n",
    "messier without resets, and the TM learns quicker with resets.\n",
    "\"\"\")\n",
    "print(\">>> tm.reset()\")\n",
    "print(\"\")\n",
    "tm.reset()\n",
    "\n",
    "\n",
    "print(\"################################################################################\")\n",
    "print(\"\")\n",
    "print(\"\"\"Send the same sequence of vectors and look at predictions made by\n",
    "temporal memory.\n",
    "\n",
    "The following prints out the active cells, predictive cells, active segments and\n",
    "winner cells.\n",
    "\n",
    "What you should notice is that the mini-columns where active state is 1\n",
    "represent the SDR for the current input pattern and the columns where predicted\n",
    "state is 1 represent the SDR for the next expected pattern.\n",
    "\"\"\")\n",
    "for inp in \"ABCDE\":\n",
    "  print(\"Input:\", inp)\n",
    "  activeColumns = dataset[inp]\n",
    "\n",
    "  print(\">>> tm.compute()\")\n",
    "  tm.compute(activeColumns, learn = False)\n",
    "\n",
    "  printStateTM(tm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
