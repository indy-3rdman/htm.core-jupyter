{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "\n",
      "A tutorial that shows some features of the Temporal Memory.\n",
      "\n",
      "This program demonstrates some basic properties of the\n",
      "Temporal Memory, in particular how it handles high-order sequences.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Creating the Temporal Memory\n",
      "Temporal Memory Parameters\n",
      "version                   = 2\n",
      "numColumns                = 2048\n",
      "cellsPerColumn            = 8\n",
      "activationThreshold       = 15\n",
      "initialPermanence         = 0.21\n",
      "connectedPermanence       = 0.3\n",
      "minThreshold              = 15\n",
      "maxNewSynapseCount        = 40\n",
      "permanenceIncrement       = 0.1\n",
      "permanenceDecrement       = 0.1\n",
      "predictedSegmentDecrement = 0.01\n",
      "maxSegmentsPerCell        = 255\n",
      "maxSynapsesPerSegment     = 255\n",
      "\n",
      "We will create a sparse representation of characters A, B, C, D, X, and Y.\n",
      "In this particular example we manually construct them, but usually you would\n",
      "use the spatial pooler to build these.\n",
      "Input A is bits at indices: [ 0 - 40 )\n",
      "Input B is bits at indices: [ 40 - 80 )\n",
      "Input C is bits at indices: [ 80 - 120 )\n",
      "Input D is bits at indices: [ 120 - 160 )\n",
      "Input X is bits at indices: [ 160 - 200 )\n",
      "Input Y is bits at indices: [ 200 - 240 )\n",
      "\n",
      "--------------------------------------------------\n",
      "Part 1. We present the sequence ABCD to the TM. The TM will eventually\n",
      "will learn the sequence and predict the upcoming characters. This can be\n",
      "measured by the prediction accuracy in Fig 1.\n",
      "N.B. In-between sequences the prediction accuracy is 0.0 as the TM does not\n",
      "output any prediction.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Once the TM has learned the sequence ABCD, we will present the individual\n",
      "characters to the TM to know its prediction. The TM outputs the columns\n",
      "that become active upon the presentation of a particular character as well\n",
      "as the columns predicted in the next time step. Here, you should see that\n",
      "A predicts B, B predicts C, C predicts D, and D does not output any\n",
      "prediction.\n",
      "N.B. Here, we are presenting individual characters, that is, a character\n",
      "deprived of context in a sequence. There is no prediction for characters\n",
      "X and Y as we have not presented them to the TM in any sequence.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- A ---\n",
      "Active cols: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
      "Predicted cols: [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "\n",
      "--- B ---\n",
      "Active cols: [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "Predicted cols: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]\n",
      "\n",
      "--- C ---\n",
      "Active cols: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]\n",
      "Predicted cols: [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]\n",
      "\n",
      "--- D ---\n",
      "Active cols: [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]\n",
      "Predicted cols: []\n",
      "\n",
      "--- X ---\n",
      "Active cols: [160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]\n",
      "Predicted cols: []\n",
      "\n",
      "--- Y ---\n",
      "Active cols: [200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]\n",
      "Predicted cols: []\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Part 2. We now present the sequence XBCY to the TM. As expected, the accuracy will\n",
      "drop until the TM learns the new sequence (Fig 2). What would be the prediction of\n",
      "the TM if presented with the sequence BC? This would depend on what character\n",
      "anteceding B. This is an important feature of high-order sequences.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "We will present again each of the characters individually to the TM, that is,\n",
      "not within any of the two sequences. When presented with character A the TM\n",
      "predicts B, B predicts C, but this time C outputs a simultaneous prediction of\n",
      "both D and Y. In order to disambiguate, the TM would require to know if the\n",
      "preceding characters were AB or XB. When presented with character X the TM\n",
      "predicts B, whereas Y and D yield no prediction.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- A ---\n",
      "Active cols: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
      "Predicted cols: [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "\n",
      "--- B ---\n",
      "Active cols: [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "Predicted cols: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]\n",
      "\n",
      "--- C ---\n",
      "Active cols: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]\n",
      "Predicted cols: [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]\n",
      "\n",
      "--- D ---\n",
      "Active cols: [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]\n",
      "Predicted cols: []\n",
      "\n",
      "--- X ---\n",
      "Active cols: [160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]\n",
      "Predicted cols: [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "\n",
      "--- Y ---\n",
      "Active cols: [200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]\n",
      "Predicted cols: []\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Part 3. Now we will present noisy inputs to the TM. We would like to see how the\n",
      "TM responds to the presence of noise and how it recovers from it. We will add\n",
      "noise to the sequence XBCY by corrupting 30% of the bits in the SDR encoding\n",
      "each character. We would expect to see a decrease in prediction accuracy as the\n",
      "TM is unable to learn the random noise in the input (Fig 3). However, this\n",
      "decrease is not significant.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Let's have a look again at the output of the TM when presented with noisy\n",
      "input (30%). Here, the noise is low enough that the TM is not affected by it,\n",
      "which would be the case if we saw 'noisy' columns being predicted when\n",
      "presented with individual characters. Thus, we could say that the TM exhibits\n",
      "resilience to noise in its input.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- A ---\n",
      "Active cols: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
      "Predicted cols: [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "\n",
      "--- B ---\n",
      "Active cols: [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "Predicted cols: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]\n",
      "\n",
      "--- C ---\n",
      "Active cols: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]\n",
      "Predicted cols: [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]\n",
      "\n",
      "--- D ---\n",
      "Active cols: [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]\n",
      "Predicted cols: []\n",
      "\n",
      "--- X ---\n",
      "Active cols: [160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]\n",
      "Predicted cols: [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "\n",
      "--- Y ---\n",
      "Active cols: [200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]\n",
      "Predicted cols: []\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Now, we will increase the noise to 60% of the bits in the characters.\n",
      "As expected, the predictive accuracy decreases (Fig 4) and 'noisy' columns are\n",
      "predicted by the TM.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "After presenting the uncorrupted sequence XBCY to the TM, we would expect to see\n",
      "the predicted noisy columns from the previous step disappear and the prediction\n",
      "accuracy return to normal. (Fig 5.)\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Part 4. We will present both sequences ABCD and XBCY randomly to the TM.\n",
      "Here, we might observe simultaneous predictions occurring when the TM is\n",
      "presented with characters D, Y, and C. For this purpose we will use a\n",
      "blank TM\n",
      "NB. Here we will not reset the TM after presenting each sequence with the\n",
      "purpose of making the TM learn different predictions for D and Y.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "We now have a look at the output of the TM when presented with the individual\n",
      "characters A, B, C, D, X, and Y. We should observe simultaneous predictions when\n",
      "presented with character D (predicting A and X), character Y (predicting A and X),\n",
      "and when presented with character C (predicting D and Y).\n",
      "--------------------------------------------------\n",
      "\n",
      "--- A ---\n",
      "Active cols: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
      "Predicted cols: [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "\n",
      "--- B ---\n",
      "Active cols: [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "Predicted cols: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]\n",
      "\n",
      "--- C ---\n",
      "Active cols: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]\n",
      "Predicted cols: [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]\n",
      "\n",
      "--- D ---\n",
      "Active cols: [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]\n",
      "Predicted cols: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]\n",
      "\n",
      "--- X ---\n",
      "Active cols: [160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]\n",
      "Predicted cols: [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "\n",
      "--- Y ---\n",
      "Active cols: [200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]\n",
      "Predicted cols: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]\n",
      "\n",
      "\n",
      "All images generated by this script are saved in your current working directory.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# HTM Community Edition of NuPIC\n",
    "# Copyright (C) 2016, Numenta, Inc.\n",
    "#               2019, David McDougall\n",
    "#\n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU Affero Public License version 3 as\n",
    "# published by the Free Software Foundation.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
    "# See the GNU Affero Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU Affero Public License\n",
    "# along with this program.  If not, see http://www.gnu.org/licenses.\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "__doc__ = \"\"\"\n",
    "A tutorial that shows some features of the Temporal Memory.\n",
    "\n",
    "This program demonstrates some basic properties of the\n",
    "Temporal Memory, in particular how it handles high-order sequences.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(1)\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from htm.bindings.sdr import SDR\n",
    "from htm.algorithms import TemporalMemory as TM\n",
    "\n",
    "print(\"--------------------------------------------------\")\n",
    "print(__doc__)\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"\")\n",
    "print(\"Creating the Temporal Memory\")\n",
    "tm = TM(\n",
    "  columnDimensions = (2048,),\n",
    "  cellsPerColumn=8,\n",
    "  initialPermanence=0.21,\n",
    "  connectedPermanence=0.3,\n",
    "  minThreshold=15,\n",
    "  maxNewSynapseCount=40,\n",
    "  permanenceIncrement=0.1,\n",
    "  permanenceDecrement=0.1,\n",
    "  activationThreshold=15,\n",
    "  predictedSegmentDecrement=0.01,\n",
    "  )\n",
    "tm.printParameters()\n",
    "\n",
    "print(\"\"\"\n",
    "We will create a sparse representation of characters A, B, C, D, X, and Y.\n",
    "In this particular example we manually construct them, but usually you would\n",
    "use the spatial pooler to build these.\"\"\")\n",
    "sparsity   = 0.02\n",
    "sparseCols = int(tm.numberOfColumns() * sparsity)\n",
    "dataset    = {inp : SDR( tm.numberOfColumns() ) for inp in \"ABCDXY\"}\n",
    "for i, inp in enumerate(\"ABCDXY\"):\n",
    "  dataset[inp].dense[ i * sparseCols : (i + 1) * sparseCols ] = 1\n",
    "  dataset[inp].dense = dataset[inp].dense # This line notifies the SDR that it's dense data has changed in-place.\n",
    "  print(\"Input\", inp, \"is bits at indices: [\",  i * sparseCols, '-', (i + 1) * sparseCols, ')')\n",
    "\n",
    "seq1 = \"ABCD\"\n",
    "seq2 = \"XBCY\"\n",
    "seqT = \"ABCDXY\"\n",
    "\n",
    "\n",
    "def trainTM(sequence, iterations, noiseLevel):\n",
    "  \"\"\"\n",
    "  Trains the TM with given sequence for a given number of time steps and level\n",
    "  of input corruption\n",
    "\n",
    "  Argument sequence   (string) Sequence of input characters.\n",
    "  Argument iterations (int)    Number of time TM will be presented with sequence.\n",
    "  Argument noiseLevel (float)  Amount of noise to be applied on the characters in the sequence.\n",
    "\n",
    "  Returns x, y\n",
    "      x is list of timestamps / step numbers\n",
    "      y is list of prediction accuracy at each step\n",
    "  \"\"\"\n",
    "  ts = 0  \n",
    "  x = []\n",
    "  y = []\n",
    "  for t in range(iterations):\n",
    "    tm.reset()\n",
    "    for inp in sequence:\n",
    "      v = SDR(dataset[inp]).addNoise( noiseLevel )\n",
    "      tm.compute( v, learn=True)\n",
    "      x.append(ts)\n",
    "      y.append( 1 - tm.anomaly )\n",
    "      ts += 1\n",
    "  return x, y\n",
    "\n",
    "\n",
    "def showPredictions():\n",
    "  \"\"\"\n",
    "  Shows predictions of the TM when presented with the characters A, B, C, D, X, and\n",
    "  Y without any contextual information, that is, not embedded within a sequence.\n",
    "  \"\"\"\n",
    "  for inp in sorted(dataset.keys()):\n",
    "    print(\"--- \" + inp + \" ---\")\n",
    "    sdr = dataset[inp]\n",
    "    tm.reset()\n",
    "    tm.compute( sdr, learn=False)\n",
    "    tm.activateDendrites(learn=False)\n",
    "    activeColumnsIndices   = [tm.columnForCell(i) for i in tm.getActiveCells().sparse]\n",
    "    predictedColumnIndices = [tm.columnForCell(i) for i in tm.getPredictiveCells().sparse]\n",
    "    print(\"Active cols: \" + str(sorted(set(activeColumnsIndices))))\n",
    "    print(\"Predicted cols: \" + str(sorted(set(predictedColumnIndices))))\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"Part 1. We present the sequence ABCD to the TM. The TM will eventually\")\n",
    "print(\"will learn the sequence and predict the upcoming characters. This can be\")\n",
    "print(\"measured by the prediction accuracy in Fig 1.\")\n",
    "print(\"N.B. In-between sequences the prediction accuracy is 0.0 as the TM does not\")\n",
    "print(\"output any prediction.\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"\")\n",
    "\n",
    "x, y = trainTM(seq1, iterations=10, noiseLevel=0.0)\n",
    "\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.ylabel(\"Prediction Accuracy\")\n",
    "plt.title(\"Fig. 1: TM learns sequence ABCD\")\n",
    "plt.savefig(\"figure_1\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"Once the TM has learned the sequence ABCD, we will present the individual\")\n",
    "print(\"characters to the TM to know its prediction. The TM outputs the columns\")\n",
    "print(\"that become active upon the presentation of a particular character as well\")\n",
    "print(\"as the columns predicted in the next time step. Here, you should see that\")\n",
    "print(\"A predicts B, B predicts C, C predicts D, and D does not output any\")\n",
    "print(\"prediction.\")\n",
    "print(\"N.B. Here, we are presenting individual characters, that is, a character\")\n",
    "print(\"deprived of context in a sequence. There is no prediction for characters\")\n",
    "print(\"X and Y as we have not presented them to the TM in any sequence.\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"\")\n",
    "\n",
    "showPredictions()\n",
    "\n",
    "print(\"\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"Part 2. We now present the sequence XBCY to the TM. As expected, the accuracy will\")\n",
    "print(\"drop until the TM learns the new sequence (Fig 2). What would be the prediction of\")\n",
    "print(\"the TM if presented with the sequence BC? This would depend on what character\")\n",
    "print(\"anteceding B. This is an important feature of high-order sequences.\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"\")\n",
    "\n",
    "x, y = trainTM(seq2, iterations=10, noiseLevel=0.0)\n",
    "\n",
    "# In this figure you can see how the TM starts making good predictions for particular\n",
    "# characters (spikes in the plot). Then, it will get half of its predictions right, which\n",
    "# correspond to the times in which is presented with character C. After some time, it\n",
    "# will learn correctly the sequence XBCY, and predict its characters accordingly.\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.ylabel(\"Prediction Accuracy\")\n",
    "plt.title(\"Fig. 2: TM learns new sequence XBCY\")\n",
    "plt.savefig(\"figure_2\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"We will present again each of the characters individually to the TM, that is,\")\n",
    "print(\"not within any of the two sequences. When presented with character A the TM\")\n",
    "print(\"predicts B, B predicts C, but this time C outputs a simultaneous prediction of\")\n",
    "print(\"both D and Y. In order to disambiguate, the TM would require to know if the\")\n",
    "print(\"preceding characters were AB or XB. When presented with character X the TM\")\n",
    "print(\"predicts B, whereas Y and D yield no prediction.\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"\")\n",
    "\n",
    "showPredictions()\n",
    "\n",
    "print(\"\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"\"\"Part 3. Now we will present noisy inputs to the TM. We would like to see how the\n",
    "TM responds to the presence of noise and how it recovers from it. We will add\n",
    "noise to the sequence XBCY by corrupting 30% of the bits in the SDR encoding\n",
    "each character. We would expect to see a decrease in prediction accuracy as the\n",
    "TM is unable to learn the random noise in the input (Fig 3). However, this\n",
    "decrease is not significant.\"\"\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"\")\n",
    "\n",
    "x, y = trainTM(seq2, iterations=50, noiseLevel=0.3)\n",
    "\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.ylabel(\"Prediction Accuracy\")\n",
    "plt.title(\"Fig. 3: Accuracy in TM with 30% noise in input\")\n",
    "plt.savefig(\"figure_3\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"Let's have a look again at the output of the TM when presented with noisy\")\n",
    "print(\"input (30%). Here, the noise is low enough that the TM is not affected by it,\")\n",
    "print(\"which would be the case if we saw 'noisy' columns being predicted when\")\n",
    "print(\"presented with individual characters. Thus, we could say that the TM exhibits\")\n",
    "print(\"resilience to noise in its input.\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"\")\n",
    "\n",
    "showPredictions()\n",
    "\n",
    "print(\"\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"Now, we will increase the noise to 60% of the bits in the characters.\")\n",
    "print(\"As expected, the predictive accuracy decreases (Fig 4) and 'noisy' columns are\")\n",
    "print(\"predicted by the TM.\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"\")\n",
    "\n",
    "x, y = trainTM(seq2, iterations=50, noiseLevel=0.6)\n",
    "\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.ylabel(\"Prediction Accuracy\")\n",
    "plt.title(\"Fig. 4: Accuracy in TM with 60% noise in input\")\n",
    "plt.savefig(\"figure_4\")\n",
    "plt.close()\n",
    "\n",
    "# Will the TM be able to forget the 'noisy' columns learned in the previous step?\n",
    "# We will present the TM with the original sequence XBCY so it forgets the 'noisy'.\n",
    "# columns.\n",
    "\n",
    "print(\"\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"\"\"After presenting the uncorrupted sequence XBCY to the TM, we would expect to see\n",
    "the predicted noisy columns from the previous step disappear and the prediction\n",
    "accuracy return to normal. (Fig 5.)\"\"\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"\")\n",
    "\n",
    "x, y = trainTM(seq2, iterations=10, noiseLevel=0.0)\n",
    "\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.ylabel(\"Prediction Accuracy\")\n",
    "plt.title(\"Fig. 5: When noise is suspended, accuracy is restored\")\n",
    "plt.savefig(\"figure_5\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"\"\"Part 4. We will present both sequences ABCD and XBCY randomly to the TM.\n",
    "Here, we might observe simultaneous predictions occurring when the TM is\n",
    "presented with characters D, Y, and C. For this purpose we will use a\n",
    "blank TM\n",
    "NB. Here we will not reset the TM after presenting each sequence with the\n",
    "purpose of making the TM learn different predictions for D and Y.\"\"\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"\")\n",
    "\n",
    "tm = TM(columnDimensions = (2048,),\n",
    "  cellsPerColumn=8,\n",
    "  initialPermanence=0.21,\n",
    "  connectedPermanence=0.3,\n",
    "  minThreshold=15,\n",
    "  maxNewSynapseCount=40,\n",
    "  permanenceIncrement=0.1,\n",
    "  permanenceDecrement=0.1,\n",
    "  activationThreshold=15,\n",
    "  predictedSegmentDecrement=0.01,\n",
    "  )\n",
    "\n",
    "for t in range(75):\n",
    "  seq = random.choice([ seq1, seq2 ])\n",
    "  for inp in seq:\n",
    "    tm.compute( dataset[inp], learn=True)\n",
    "\n",
    "print(\"\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"We now have a look at the output of the TM when presented with the individual\")\n",
    "print(\"characters A, B, C, D, X, and Y. We should observe simultaneous predictions when\")\n",
    "print(\"presented with character D (predicting A and X), character Y (predicting A and X),\")\n",
    "print(\"and when presented with character C (predicting D and Y).\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"\")\n",
    "\n",
    "showPredictions()\n",
    "\n",
    "print(\"\")\n",
    "print(\"All images generated by this script are saved in your current working directory.\")\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
